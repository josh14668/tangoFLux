{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPN9hYUfbVn+iUp4Mp6TpLa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# **Baseline System**"],"metadata":{"id":"SAdLECmO7Y6x"}},{"cell_type":"markdown","source":["**Install required packages:**"],"metadata":{"id":"UaEot-6wyO6H"}},{"cell_type":"code","source":["\"\"\"\n","DO NOT MODIFY THIS BLOCK.\n","\"\"\"\n","\n","# Install packages for template code.\n","! pip install GitPython gdown==5.1.0\n","# Install packages for Baseline Model.\n","# If this cause a 'pydevd_plugins' error, simply RESTART the SESSION to solve the problem.\n","! pip install librosa==0.9.2 pytorch-lightning==2.1.1 transformers==4.30.2 einops==0.7.0 torchlibrosa==0.0.9 ftfy==6.1.1 braceexpand==0.1.7 webdataset==0.2.75 wget==3.2 timm==0.4.12 wandb taming-transformers-rom1504==0.0.6"],"metadata":{"id":"sL8OpD11zf45","colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"status":"ok","timestamp":1745987313684,"user_tz":240,"elapsed":5938,"user":{"displayName":"Kahlia Gronthos","userId":"09084025027598369343"}},"outputId":"8dac537a-8d2b-43c8-829a-7c184f392c73"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: GitPython in /usr/local/lib/python3.11/dist-packages (3.1.44)\n","Requirement already satisfied: gdown==5.1.0 in /usr/local/lib/python3.11/dist-packages (5.1.0)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown==5.1.0) (4.13.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown==5.1.0) (3.18.0)\n","Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown==5.1.0) (2.32.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown==5.1.0) (4.67.1)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from GitPython) (4.0.12)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->GitPython) (5.0.2)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown==5.1.0) (2.7)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown==5.1.0) (4.13.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown==5.1.0) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown==5.1.0) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown==5.1.0) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown==5.1.0) (2025.1.31)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown==5.1.0) (1.7.1)\n","Requirement already satisfied: librosa==0.9.2 in /usr/local/lib/python3.11/dist-packages (0.9.2)\n","Requirement already satisfied: pytorch-lightning==2.1.1 in /usr/local/lib/python3.11/dist-packages (2.1.1)\n","Requirement already satisfied: transformers==4.30.2 in /usr/local/lib/python3.11/dist-packages (4.30.2)\n","Requirement already satisfied: einops==0.7.0 in /usr/local/lib/python3.11/dist-packages (0.7.0)\n","Requirement already satisfied: torchlibrosa==0.0.9 in /usr/local/lib/python3.11/dist-packages (0.0.9)\n","Requirement already satisfied: ftfy==6.1.1 in /usr/local/lib/python3.11/dist-packages (6.1.1)\n","Requirement already satisfied: braceexpand==0.1.7 in /usr/local/lib/python3.11/dist-packages (0.1.7)\n","Requirement already satisfied: webdataset==0.2.75 in /usr/local/lib/python3.11/dist-packages (0.2.75)\n","Requirement already satisfied: wget==3.2 in /usr/local/lib/python3.11/dist-packages (3.2)\n","Requirement already satisfied: timm==0.4.12 in /usr/local/lib/python3.11/dist-packages (0.4.12)\n","Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.10)\n","Requirement already satisfied: taming-transformers-rom1504==0.0.6 in /usr/local/lib/python3.11/dist-packages (0.0.6)\n","Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa==0.9.2) (3.0.1)\n","Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from librosa==0.9.2) (2.0.2)\n","Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from librosa==0.9.2) (1.15.2)\n","Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from librosa==0.9.2) (1.6.1)\n","Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.11/dist-packages (from librosa==0.9.2) (1.4.2)\n","Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.11/dist-packages (from librosa==0.9.2) (4.4.2)\n","Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.11/dist-packages (from librosa==0.9.2) (0.4.3)\n","Requirement already satisfied: numba>=0.45.1 in /usr/local/lib/python3.11/dist-packages (from librosa==0.9.2) (0.60.0)\n","Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.11/dist-packages (from librosa==0.9.2) (0.13.1)\n","Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa==0.9.2) (1.8.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from librosa==0.9.2) (24.2)\n","Requirement already satisfied: torch>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning==2.1.1) (2.6.0+cu124)\n","Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning==2.1.1) (4.67.1)\n","Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning==2.1.1) (6.0.2)\n","Requirement already satisfied: fsspec>2021.06.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning==2.1.1) (2025.3.2)\n","Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning==2.1.1) (1.7.1)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning==2.1.1) (4.13.2)\n","Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning==2.1.1) (0.14.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.2) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.2) (0.30.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.2) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.2) (2.32.3)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.2) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.2) (0.5.3)\n","Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.11/dist-packages (from ftfy==6.1.1) (0.2.13)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm==0.4.12) (0.21.0+cu124)\n","Requirement already satisfied: omegaconf>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from taming-transformers-rom1504==0.0.6) (2.3.0)\n","Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n","Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n","Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n","Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\n","Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n","Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.3)\n","Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.27.0)\n","Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning==2.1.1) (3.11.15)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.45.1->librosa==0.9.2) (0.43.0)\n","Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from omegaconf>=2.0.0->taming-transformers-rom1504==0.0.6) (4.9.3)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.1)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.30.2) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.30.2) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.30.2) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.30.2) (2025.1.31)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.19.1->librosa==0.9.2) (3.6.0)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.10.2->librosa==0.9.2) (1.17.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->pytorch-lightning==2.1.1) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->pytorch-lightning==2.1.1) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->pytorch-lightning==2.1.1) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->pytorch-lightning==2.1.1) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->pytorch-lightning==2.1.1) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->pytorch-lightning==2.1.1) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->pytorch-lightning==2.1.1) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->pytorch-lightning==2.1.1) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->pytorch-lightning==2.1.1) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->pytorch-lightning==2.1.1) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->pytorch-lightning==2.1.1) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->pytorch-lightning==2.1.1) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->pytorch-lightning==2.1.1) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->pytorch-lightning==2.1.1) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->pytorch-lightning==2.1.1) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->pytorch-lightning==2.1.1) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->pytorch-lightning==2.1.1) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.12.0->pytorch-lightning==2.1.1) (1.3.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->timm==0.4.12) (11.2.1)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==2.1.1) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==2.1.1) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==2.1.1) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==2.1.1) (1.6.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==2.1.1) (6.4.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==2.1.1) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==2.1.1) (1.20.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa==0.9.2) (2.22)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.12.0->pytorch-lightning==2.1.1) (3.0.2)\n"]}]},{"cell_type":"code","source":["\"\"\"\n","DO NOT MODIFY THIS BLOCK.\n","\"\"\"\n","from typing import List,Dict,Tuple\n","from numpy import ndarray\n","\n","from abc import ABC, abstractmethod\n","from tqdm import tqdm\n","from IPython import display"],"metadata":{"id":"edcEVLNjzmW8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Mount Drive, set up path, define prompts, etc.**"],"metadata":{"id":"4y2Dotgm1qsD"}},{"cell_type":"code","source":["\"\"\"\n","DO NOT MODIFY THIS BLOCK.\n","This block is used for every submission.\n","\"\"\"\n","import os\n","import tqdm\n","import soundfile as sf\n","from google.colab import drive\n","\n","drive.mount('/content/gdrive')\n","#ROOT_PATH = \"/content/gdrive/MyDrive/DCASE2024-T7\"\n","ROOT_PATH = \"/content/gdrive/MyDrive/ML_Project/baseline\"\n","\n","\n","# text_prompts_list = [\"a buzzer is ringing with water in the background\",\n","#     \"a pig is grunting with water in the background\",\n","#     \"an alarm of a car door stayin open is ringin with crowd in the background\",\n","#     \"a small dog is whining with water in the background\",\n","#     \"a car horn is honking with crowd in the background\",\n","#     \"a baby is laughing with crowd in the background\",\n","#     \"a burglar alarm is ringing with traffic in the background\"] # Example text prompts from Dev. Set.\n","\n","# function to read text_prompts_list from 'caption' column of a CSV file\n","def read_text_prompts_from_csv(filepath: str) -> List[str]:\n","    \"\"\"Reads text prompts from a CSV file.\n","\n","    Args:\n","        filepath (str): path to the CSV file.\n","\n","    Returns:\n","        text_prompts (list of strings): List of text prompts.\n","\n","    \"\"\"\n","    import pandas as pd\n","    assert os.path.exists(filepath), f\"File not found: {filepath}\"\n","    df = pd.read_csv(filepath)\n","    return df['caption'].tolist()\n","\n","#text_prompts_list = read_text_prompts_from_csv(os.path.join(ROOT_PATH, 'dataset/dev/caption.csv')) # organizers will update this\n","text_prompts_list = read_text_prompts_from_csv(os.path.join(ROOT_PATH, 'development_dataset/captions.csv')) # organizers will update this\n","\n","\n","SR = 32000  # audio sample-rate in Hz\n","duration = 4  # audio duratio in seconds\n","submission_idx = 0  # organizers will update this\n","\n","save_folder = f'submission-{submission_idx:02d}'\n","print(save_folder)\n","os.makedirs(os.path.join(ROOT_PATH, save_folder), exist_ok=True)  # set to be False so that we won't overwrite.\n"],"metadata":{"id":"9kO5PS878Pwe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745987327560,"user_tz":240,"elapsed":967,"user":{"displayName":"Kahlia Gronthos","userId":"09084025027598369343"}},"outputId":"835569e4-2f2b-40a4-bdf6-d1d308308ee1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","submission-00\n"]}]},{"cell_type":"markdown","source":["**Abstract class for sound synthesis model**"],"metadata":{"id":"We8x9i3ayXL4"}},{"cell_type":"code","source":["\"\"\"\n","DO NOT MODIFY THIS BLOCK.\n","YOU SHOULD SUBCLASS `SoundSynthesisModel` to wrap your model.\n","\"\"\"\n","class SoundSynthesisModel(ABC):\n","    @abstractmethod\n","    def synthesize_sounds(self, text_prompts: List[str]) -> Dict[str, ndarray]:\n","        \"\"\"Synthesize sound examples that corresponds to the given text prompts respectively.\n","\n","        Args:\n","            text_prompts (list of strings): text prompts in string enclosed in a list.\n","\n","        Return:\n","            sound_samples (dict): A dictionary with text prompts as keys and the corresponding sound samples as values.\n","                Each value should be a 1-dim array (mono signal) with sample_rate=32000.\n","                If your model is not working at 32,000Hz, please add a resampling logic within this method.\n","\n","        \"\"\"\n","        pass"],"metadata":{"id":"EjjDCuJ8uOjO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Utility functions for pulling in the pretrained models and checkpoints required for baseline system.**"],"metadata":{"id":"JTGP00a3v3p1"}},{"cell_type":"code","source":["'''\n","DO NOT MODIFY THIS BLOCK.\n","'''\n","\n","import os\n","import gdown\n","from git import Repo\n","\n","\n","def check_download_file_info(filename: str, shared_url: str, relative_dir: str, url_prefix: str) -> None:\n","  if not shared_url.startswith(url_prefix):\n","    raise ValueError(f\"Invalid url: {shared_url}.\\nMake sure the url is valid.\\nIt should start with \\'{url_prefix}\\'.\")\n","  if '/' in filename:\n","    raise ValueError(f\"Invalid filename: {filename}.\\nMake sure the filename does not start with \\'/\\'.\")\n","  if relative_dir.startswith('/'):\n","    raise ValueError(f\"Invalid relative_dir: {relative_dir}.\\nMake sure the relative_dir is not an absolute path.\")\n","\n","\n","def google_drive_download(filename: str, shared_url: str, relative_dir: str) -> None:\n","  check_download_file_info(filename, shared_url, relative_dir, 'https://drive.google.com')\n","  os.makedirs(os.path.join(baseline_dir, relative_dir), exist_ok=True)\n","  print(f'Downloading \\'{filename}\\' from gdrive to {os.path.join(baseline_dir, relative_dir, filename)}')\n","  gdown.download(url=shared_url, output=os.path.join(baseline_dir, relative_dir, filename),\n","                 quiet=False, fuzzy=True)\n","\n","\n","def wget_download(filename: str, shared_url: str, relative_dir: str) -> None:\n","  check_download_file_info(filename, shared_url, relative_dir, 'https://')\n","  os.makedirs(os.path.join(baseline_dir, relative_dir), exist_ok=True)\n","\n","  import subprocess\n","  from IPython.display import display, clear_output\n","  import time\n","\n","  command = ['wget', shared_url, '-O', os.path.join(baseline_dir, relative_dir, filename), '-v']\n","\n","  process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n","\n","  while True:\n","    output = process.stdout.readline()\n","    if output == '' and process.poll() is not None:\n","        break\n","    if output:\n","        # Clear the previous output\n","        clear_output(wait=True)\n","        # Display the new output\n","        display(output.strip())\n","    time.sleep(0.1)  # Add a slight delay to reduce flickering\n","\n","  rc = process.poll()\n","  if rc == 0:\n","      print(f\"Download completed successfully (filename: {filename}).\")\n","  else:\n","      print(f\"Download failed with return code {rc} (filename: {filename}).\")\n","\n","\n","\n","def git_clone_checkout(output_dir: str, url: str, branch: str, commit_sha: str) -> None:\n","  if not url.startswith('https://'):\n","    raise ValueError(f\"Invalid url: {url}.\\nMake sure the url is valid.\\nIt should start with \\'https://\\'\")\n","  # Clone the repository (This will clone the default branch)\n","  os.makedirs(os.path.join(baseline_dir, output_dir), exist_ok=True)\n","  repo = Repo.clone_from(url, os.path.join(baseline_dir, output_dir))\n","  # Checkout the specific branch\n","  repo.git.checkout(branch)\n","  # Checkout the specific commit\n","  repo.git.checkout(commit_sha)\n","\n","\n","def unpack_file(file_path: str, output_dir: str) -> None:\n","  import shutil\n","\n","  if not os.path.exists(os.path.join(baseline_dir, file_path)):\n","    raise ValueError(f\"File not found: {os.path.join(baseline_dir, file_path)}\")\n","  file_format = '.'.join(os.path.basename(file_path).split('.')[1:])\n","  print(f\"Unpacking file {os.path.basename(file_path)}...\")\n","\n","  if file_format == 'tar':\n","    shutil.unpack_archive(os.path.join(baseline_dir, file_path), os.path.join(baseline_dir, output_dir), format='tar')\n","  elif file_format == 'tar.gz':\n","    shutil.unpack_archive(os.path.join(baseline_dir, file_path), os.path.join(baseline_dir, output_dir), format='gztar')\n","  elif file_format == 'tar.xz':\n","    shutil.unpack_archive(os.path.join(baseline_dir, file_path), os.path.join(baseline_dir, output_dir), format='xztar')\n","  elif file_format == 'zip':\n","    shutil.unpack_archive(os.path.join(baseline_dir, file_path), os.path.join(baseline_dir, output_dir), format='zip')\n","  else:\n","    raise ValueError(f'Format {file_format} is not supported. Use .tar, .tar.gz, .tar.xz, or .zip format.')"],"metadata":{"id":"YXHGEmJRvg74"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Prepare a folder where the implementation of the baseline model will be stored:**"],"metadata":{"id":"wtlOj8MuzI2S"}},{"cell_type":"code","source":["import os\n","baseline_dir:str = f'{ROOT_PATH}/baseline_implementation'\n","os.makedirs(baseline_dir,exist_ok=True)"],"metadata":{"id":"VlTTjjJFvPbA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Clone the baseline code repository and download checkpoints (pretrained models and audio synthesis components):**"],"metadata":{"id":"pph4dT-bwQtT"}},{"cell_type":"code","source":["'''\n","DO NOT MODIFY THIS BLOCK.\n","'''\n","\n","git_clone_checkout('./AudioLDM-training-finetuning', 'https://github.com/DCASE2024-Task7-Sound-Scene-Synthesis/AudioLDM-training-finetuning.git',\n","                   'main', 'a6b15e86c3d042832dee08a94beb11819b297e39')\n","\n","download_files_google_list = [\n","    # ('checkpoints.tar', 'https://drive.google.com/file/d/1T6EnuAHIc8ioeZ9kB1OZ_WGgwXAVGOZS/view?usp=sharing',\n","    #  'AudioLDM-training-finetuning/data')\n","]\n","\n","download_files_wget_list = [\n","    ('checkpoints.tar', 'https://www.dropbox.com/scl/fi/he6rqr24y1pc3s94lm8tc/checkpoints.tar?rlkey=5yu046f5uvdijq8eor77ej4fx&dl=0',\n","     'AudioLDM-training-finetuning/data'),\n","    ('audioldm-m-full.ckpt', 'https://zenodo.org/records/7884686/files/audioldm-m-full.ckpt',\n","     'AudioLDM-training-finetuning/data/checkpoints')\n","]\n","\n","# for filename, shared_url, relative_dir in download_files_google_list:\n","#   google_drive_download(filename, shared_url, relative_dir)\n","for filename, shared_url, relative_dir in download_files_wget_list:\n","  wget_download(filename, shared_url, relative_dir)\n","\n","unpack_file('AudioLDM-training-finetuning/data/checkpoints.tar', 'AudioLDM-training-finetuning/data')"],"metadata":{"id":"GdfQJjhcv_U5","colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"outputId":"6c4e4030-fbfc-4a6d-febf-a3dc8fa58d58"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["''"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Download completed successfully (filename: audioldm-m-full.ckpt).\n","Unpacking file checkpoints.tar...\n"]}]},{"cell_type":"markdown","source":["**Define the baseline model class:**"],"metadata":{"id":"CTlxXIVDxVtu"}},{"cell_type":"code","source":["'''\n","DO NOT MODIFY THIS BLOCK.\n","'''\n","\n","import yaml\n","import torch\n","import importlib\n","import librosa\n","import sys\n","from torch.utils.data import DataLoader\n","from pytorch_lightning import seed_everything\n","\n","sys.path.append(os.path.join(baseline_dir, 'AudioLDM-training-finetuning'))\n","from audioldm_train.utilities.data.dataset import AudioDataset\n","from audioldm_train.utilities.model_util import instantiate_from_config\n","\n","\n","def get_input_with_key(batch, k):\n","    fname, text, label_indices, waveform, stft, fbank = (\n","        batch[\"fname\"],\n","        batch[\"text\"],\n","        batch[\"label_vector\"],\n","        batch[\"waveform\"],\n","        batch[\"stft\"],\n","        batch[\"log_mel_spec\"],\n","    )\n","\n","    ret = {}\n","\n","    ret[\"fbank\"] = (\n","        fbank.unsqueeze(1).to(memory_format=torch.contiguous_format).float()\n","    )\n","    ret[\"stft\"] = stft.to(memory_format=torch.contiguous_format).float()\n","    ret[\"waveform\"] = waveform.to(memory_format=torch.contiguous_format).float()\n","    ret[\"text\"] = list(text)\n","    ret[\"fname\"] = fname\n","\n","    for key in batch.keys():\n","        if key not in ret.keys():\n","            ret[key] = batch[key]\n","\n","    return ret[k]\n","\n","def find_loudest_segment(audio: np.ndarray, sr: int, segment_length: int = 4, hop_length_sec: float = 2.0) -> np.ndarray:\n","    \"\"\"\n","    Find the loudest segment in an audio waveform using Librosa's framing and RMS features.\n","\n","    Parameters:\n","    - audio (np.ndarray): The audio waveform as a NumPy ndarray.\n","    - sr (int): The sampling rate of the audio waveform in Hz.\n","    - segment_length (int): The length of the segment to find in whole seconds.\n","    - hop_length_sec (float): The hop length for segment calculation in seconds.\n","\n","    Returns:\n","    - np.ndarray: The loudest segment of the audio waveform.\n","    \"\"\"\n","    hop_length_samples = int(sr * hop_length_sec)\n","    frame_length_samples = int(sr * segment_length)\n","\n","    rms_values = librosa.feature.rms(y=audio, frame_length=frame_length_samples, hop_length=hop_length_samples, center=False)\n","\n","    max_rms_index = np.argmax(rms_values)\n","\n","    start_sample = max_rms_index * hop_length_samples\n","    end_sample = start_sample + frame_length_samples\n","\n","    loudest_segment = audio[start_sample:end_sample]\n","\n","    return loudest_segment\n","\n","\n","class BaseLineModel(SoundSynthesisModel):\n","    def __init__(self) -> None:\n","        super().__init__()\n","\n","        self.sr: int = 32000 # sampling rate\n","        self.duration: int = 4 # audio length in seconds\n","        self.batch_size: int = 8 # batch size in int\n","        self.loudest_hop_len: float = 2.0 # hop size for function 'find_loudest_segment' in seconds\n","\n","        config_yaml_path = os.path.join(repo_dir,\n","                                        'audioldm_train/config/2023_08_23_reproduce_audioldm/audioldm_original_medium.yaml')\n","        reload_from_ckpt = os.path.join(repo_dir, 'data/checkpoints/audioldm-m-full.ckpt')\n","        self.configs = yaml.load(open(config_yaml_path, \"r\"), Loader=yaml.FullLoader)\n","        self.configs[\"reload_from_ckpt\"] = reload_from_ckpt\n","        clap_ckpt_path = self.configs[\"model\"][\"params\"][\"cond_stage_config\"][\"film_clap_cond1\"][\"params\"][\"pretrained_path\"]\n","        self.configs[\"model\"][\"params\"][\"cond_stage_config\"][\"film_clap_cond1\"][\"params\"][\"pretrained_path\"] = os.path.join(\n","            baseline_dir,'AudioLDM-training-finetuning', clap_ckpt_path)\n","\n","        if \"seed\" in self.configs.keys():\n","            seed_everything(self.configs[\"seed\"])\n","        else:\n","            print(\"SEED EVERYTHING TO 0\")\n","            seed_everything(0)\n","        if \"precision\" in self.configs.keys():\n","            torch.set_float32_matmul_precision(self.configs[\"precision\"])\n","\n","        self.latent_diffusion = instantiate_from_config(self.configs[\"model\"])\n","        checkpoint = torch.load(self.configs[\"reload_from_ckpt\"])\n","        self.latent_diffusion.load_state_dict(checkpoint[\"state_dict\"], strict=False)\n","\n","    @torch.no_grad()\n","    def synthesize_sounds(self, text_prompts: List[str]) -> Dict[str, ndarray]:\n","        audio_list:List[ndarray] = list()\n","\n","        dataset_json = {\"data\": [{'wav': '', 'caption': caption} for caption in text_prompts]}\n","\n","        if \"dataloader_add_ons\" in self.configs[\"data\"].keys():\n","            dataloader_add_ons = self.configs[\"data\"][\"dataloader_add_ons\"]\n","        else:\n","            dataloader_add_ons = []\n","\n","        val_dataset = AudioDataset(\n","            self.configs, split=\"test\", add_ons=dataloader_add_ons, dataset_json=dataset_json\n","        )\n","        val_loader = DataLoader(\n","            val_dataset,\n","            batch_size=self.batch_size,\n","        )\n","\n","        guidance_scale = self.configs[\"model\"][\"params\"][\"evaluation_params\"][\n","            \"unconditional_guidance_scale\"\n","        ]\n","        ddim_sampling_steps = self.configs[\"model\"][\"params\"][\"evaluation_params\"][\n","            \"ddim_sampling_steps\"\n","        ]\n","        n_candidates_per_samples = self.configs[\"model\"][\"params\"][\"evaluation_params\"][\n","            \"n_candidates_per_samples\"\n","        ]\n","\n","        self.latent_diffusion.eval()\n","        self.latent_diffusion = self.latent_diffusion.cuda()\n","\n","        waveforms_dict = self.generate_sample(\n","            val_loader,\n","            unconditional_guidance_scale=guidance_scale,\n","            ddim_steps=ddim_sampling_steps,\n","            n_gen=n_candidates_per_samples,\n","            sampling_rate=self.configs[\"variables\"][\"sampling_rate\"],\n","        )\n","\n","        return_audio_dict = {}\n","        for text_prompt, waveform in waveforms_dict.items():\n","          # resample the audio if the model doesn't output 32,000Hz waveform\n","          if not self.configs['variables']['sampling_rate'] == self.sr:\n","            waveform = librosa.resample(waveform, orig_sr=self.configs['variables']['sampling_rate'],\n","                                        target_sr=self.sr)\n","          # pad or chop the audio if the model doesn't output 4-second audio\n","          if len(waveform) < self.sr * self.duration:\n","              waveform = np.pad(waveform, (0, (self.sr * self.duration)-len(waveform)), 'constant', constant_values=0)\n","          elif len(waveform) > self.sr * self.duration:\n","              waveform = find_loudest_segment(waveform, sr=self.sr,\n","                                              segment_length=self.duration, hop_length_sec=self.loudest_hop_len)\n","          return_audio_dict[text_prompt] = waveform\n","\n","        assert len(return_audio_dict) == len(text_prompts), f\"return_dict {len(return_audio_dict)} prompts {len(text_prompts)}\"\n","\n","        return return_audio_dict\n","\n","    @torch.no_grad()\n","    def generate_sample(\n","        self,\n","        batchs,\n","        ddim_steps=200,\n","        ddim_eta=1.0,\n","        x_T=None,\n","        n_gen=1,\n","        unconditional_guidance_scale=1.0,\n","        unconditional_conditioning=None,\n","        use_plms=False,\n","        **kwargs,\n","    ) -> Dict[str, np.ndarray]:\n","        # Generate n_gen times and select the best\n","        # Batch: audio, text, fnames\n","        assert x_T is None\n","        try:\n","            batchs = iter(batchs)\n","        except TypeError:\n","            raise ValueError(\"The first input argument should be an iterable object\")\n","\n","        if use_plms:\n","            assert ddim_steps is not None\n","\n","        use_ddim = ddim_steps is not None\n","\n","        model = self.latent_diffusion\n","        waveforms = {}\n","\n","        with model.ema_scope(\"Plotting\"):\n","            for i, batch in enumerate(batchs):\n","                z, c = model.get_input(\n","                    batch,\n","                    model.first_stage_key,\n","                    unconditional_prob_cfg=0.0,\n","                )\n","\n","                c = model.filter_useful_cond_dict(c)\n","\n","                text = get_input_with_key(batch, \"text\")\n","\n","                # Generate multiple samples\n","                batch_size = z.shape[0] * n_gen\n","\n","                # Generate multiple samples at a time and filter out the best\n","                # The condition to the diffusion wrapper can have many format\n","                for cond_key in c.keys():\n","                    if isinstance(c[cond_key], list):\n","                        for i in range(len(c[cond_key])):\n","                            c[cond_key][i] = torch.cat([c[cond_key][i]] * n_gen, dim=0)\n","                    elif isinstance(c[cond_key], dict):\n","                        for k in c[cond_key].keys():\n","                            c[cond_key][k] = torch.cat([c[cond_key][k]] * n_gen, dim=0)\n","                    else:\n","                        c[cond_key] = torch.cat([c[cond_key]] * n_gen, dim=0)\n","\n","                text = text * n_gen\n","\n","                if unconditional_guidance_scale != 1.0:\n","                    unconditional_conditioning = {}\n","                    for key in model.cond_stage_model_metadata:\n","                        model_idx = model.cond_stage_model_metadata[key][\"model_idx\"]\n","                        unconditional_conditioning[key] = model.cond_stage_models[\n","                            model_idx\n","                        ].get_unconditional_condition(batch_size)\n","\n","                fnames = list(get_input_with_key(batch, \"fname\"))\n","\n","                samples, _ = model.sample_log(\n","                    cond=c,\n","                    batch_size=batch_size,\n","                    x_T=x_T,\n","                    ddim=use_ddim,\n","                    ddim_steps=ddim_steps,\n","                    eta=ddim_eta,\n","                    unconditional_guidance_scale=unconditional_guidance_scale,\n","                    unconditional_conditioning=unconditional_conditioning,\n","                    use_plms=use_plms,\n","                )\n","\n","                mel = model.decode_first_stage(samples)\n","\n","                waveform = model.mel_spectrogram_to_waveform(\n","                    mel, bs=None, name=fnames, save=False\n","                )\n","\n","                if n_gen > 1:\n","                    try:\n","                        best_index = []\n","                        similarity = model.clap.cos_similarity(\n","                            torch.FloatTensor(waveform).squeeze(1), text\n","                        )\n","                        for i in range(z.shape[0]):\n","                            candidates = similarity[i :: z.shape[0]]\n","                            max_index = torch.argmax(candidates).item()\n","                            best_index.append(i + max_index * z.shape[0])\n","\n","                        waveform = waveform[best_index]\n","\n","                    except Exception as e:\n","                        print(\"Warning: while calculating CLAP score (not fatal), \", e)\n","\n","                text = text[:len(text)//n_gen]\n","                assert len(text) == waveform.shape[0], f'{len(text)}, {waveform.shape}'\n","                for idx, text_prompt in enumerate(text):\n","                  assert not waveforms.get(text_prompt, False)\n","                  waveforms[text_prompt] = np.squeeze(waveform[idx], axis=0)\n","\n","        return waveforms\n","\n","\n","# DO NOT change the working directory in your own code.\n","# This is only for demonstration purpose.\n","repo_dir = os.path.join(baseline_dir, 'AudioLDM-training-finetuning')\n","os.chdir(repo_dir)"],"metadata":{"id":"SNQS4LNcxFic"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Exectute the pipeline for generating and validating audio clips using the baseline model:**"],"metadata":{"id":"vXk0wPwxx2g0"}},{"cell_type":"code","source":["import time\n","import IPython.display as ipd\n","import numpy as np\n","\n","\n","def check_srcs_dict(srcs_dict: Dict[str, np.ndarray], text_prompts_list: List[str], duration: int, SR: int) -> None:\n","    \"\"\" Check if the return value of synthesize_sounds method is valid.\n","\n","    Args:\n","        srcs_dict (dict): the return value of synthesize_sounds method.\n","        text_prompts_list (list of strings): list of text prompts.\n","        duration (int): duration of the audio in seconds.\n","        SR (int): sample rate of the audio.\n","\n","    Returns:\n","        None\n","    \"\"\"\n","    assert isinstance(srcs_dict, dict), \"The return value of synthesize_sounds method should be a dictionary.\"\n","    assert all(isinstance(k, str) for k in srcs_dict.keys()), \"The keys of dictionary, the return value of \\'synthesize_sounds\\' method, should be strings (corresponding text prompt).\"\n","    assert all(isinstance(v, np.ndarray) for v in srcs_dict.values()), \"The values of dictionary, the return value of \\'synthesize_sounds\\' method, should be numpy arrays (audio waveform).\"\n","    assert list(srcs_dict.keys()) == text_prompts_list, \"The keys of dictionary, the return value of \\'synthesize_sounds\\' method, should match the input text prompts.\"\n","    for _, src in srcs_dict.items():\n","        assert src.ndim == 1, \"The audio waveform should be mono.\"\n","        assert len(src) == int(duration * SR), \"The audio waveform should be 4 seconds long.\"\n","\n","\n","start_time = time.time() # measure total inference time\n","\n","fss_model = BaseLineModel()\n","\n","srcs_dict = fss_model.synthesize_sounds(text_prompts_list)\n","check_srcs_dict(srcs_dict, text_prompts_list, duration, SR)\n","\n","os.makedirs(os.path.join(ROOT_PATH, baseline_dir, 'output'), exist_ok=True)\n","for src_text, src in tqdm.tqdm(srcs_dict.items()):\n","    _filepath = os.path.join(ROOT_PATH, baseline_dir, 'output', f\"{src_text}.wav\")\n","    src = src / np.max(np.abs(src)) # normalize the energy of the generation output\n","    sf.write(_filepath, src, SR, subtype='PCM_16')\n","\n","inference_time = time.time() - start_time\n","print(\"Total inference time: \", inference_time)\n","\n","print('Listen to the generated sound...')\n","print(f'- prompt: {text_prompts_list[0]}')\n","ipd.Audio(srcs_dict[text_prompts_list[0]], rate=SR) # listen to the generated result"],"metadata":{"id":"_DXcthRNx4fN"},"execution_count":null,"outputs":[]}]}